## 

W.R. Ashby identifies variety primarily as the number of distinct configurations or states that a system can assume, treating it as fundamentally equivalent to information. As established in our conversation history, Ashby measures variety using logarithmic units (bits), specifically the logarithm to the base 2 of the number of possible states. In contrast, drawing from the sources, Ashby describes Shannon’s concept of entropy (often denoted as H) as the negative of the amount of information contained in a message. While variety focuses on the raw count of available states, Shannon’s entropy incorporates the probability distribution over an ensemble of possible messages or states <sup>1</sup>.    

The distinctions between these concepts in Ashby’s thought include:     
• Choice vs. Uncertainty: Ashby views variety as a measure of the choice among alternatives; for example, a message selected from 100 possible alternatives contains a specific amount of information regardless of its length. Shannon's entropy measures the uncertainty or the average information content per symbol, where information from independent sources accumulates additively while their probabilities multiply, necessitating a logarithmic scale <sup>2</sup>.     

• The Absolute System Context: Ashby notes that an absolute system (a deterministic machine) cannot generate its own variety; the output of information is exactly equal to the input from its parameters <sup>3</sup>. He observes that once a message or state is formed, subsequent operations in a noiseless system can deprive it of information but never augment it <sup>4,5</sup>.     

• Contradiction with Physical Entropy: A crucial distinction Ashby makes is that the "entropy of probabilities" in a dynamic absolute system can only decrease as the system moves toward equilibrium. As lines of behavior in a field converge toward a resting state or cycle, these convergent junctions act as "sinks" where information is destroyed. This behavior is noted by Ashby to be directly contrary to the entropy of physics, which tends to increase.    

• Mathematical Synthesis: Ashby utilizes Shannon's mathematical framework (H=−∑plogp) to calculate the average variety of a system when its various states have different probabilities of occurring. He defines the initial information in a conjoined system as the sum of the separate informations of its independent parts.    

Ultimately, while variety represents the potential range of a system's behavior, Shannon’s entropy is used by Ashby to quantify the degree of organization or "orderliness" within the system’s probabilistic state-space.   

Footnotes:
<sup>1</sup> 1948-1949
(94/277) 2518
"create at the receiving end of a communicating system an "enduring state completely characterized in terms of its own "past then I cease to convey information. If I said "one of those elaborate Christmas or birthday messages "favoured by our telegraph companies, containing a "large amount of sentimental verbiage coded in terms "of a number from one to one hundred, then the "amount of information which I am sending is to be "measured by the choice among the hundred alternatives "& has nothing to do with the length of the transcribed "'message'." ..... "We thus see that the notion of 'message' "involves the two fundamental ideas of statistical "mechanics: the ensemble, & the probability "distribution over the ensemble." [Consider the "information value of a congratulatory cable when we discover "that this man always sends that cable]. "One needs "a little more explanation for the fact that it involves "entropy as well. Entropy here appears as the "negative of the amount of information contained in the "message. "It may not be obvious at first sight why.     

<sup>2</sup> 1948-1949
the notion of logarithm occurs in the measurement of the amount of information. Let me point out that information from two independent sources accumulates additively while their probabilities are multiplied & that a variable which increases additively while another increases multiplicatively is, except for a constant factor, the logarithm......     
Perhaps we may exemplify this another way. Let A be a quantity to be measured, lying on the range between zero & B. Let C be the error within "which A can be measured. Let us write both A & C on the binary scale. Then the number of digits in A will be log2A, & the number of digits in C will be log2C. The difference between these two numbers represents the number of digits. Each digit has two possible values & only two= namely zero and one. It follows that the number of significant decisions between two alternatives which we make in our measurement is the logarithm to the base 2 of the ration A divided by C. If, however, we consider A as the size of our universe and C as the size of a certain zone.....     

<sup>3</sup> 1948-1949
way to achieve full extinction in the presence of the stimulation of starting an experiment is to.... His paper contains much factual material on 'extinction', and should be consulted if this subject is being considered. Summary: Facts on learning.

29 Oct 50      
The fact that the number of fields shown by an absolute system is exactly equal to the number of combinations of values provided by its parameters can be re-stated in terms of information theory if we regard the parameters as 'input' and the field-characteristics as 'output'. We then say that the output of information is exactly equal to the input of it. For this to be so, it is necessary + sufficient that the system be absolute (Shannon, p. 2992).     

<sup>4</sup> 1948-1949
(96/277) 2520     
of confusion, the number of decisions will be the "logarithm of the probability that a measurement lies in a certain zone of confusion, when the probability of the entire universe is 1. This probability is taken to the base 2, but a change of base in our "probability system only changes the measure of the amount of information by a constant factor. Thus, "essentially, amount of information is the negative of entropy....  Once a message has been formed, a subsequent operation on it may deprive it of some of its information, "but can never augment it. 

3 June 48     
How does the homeostat compare with the ACE & ENIAC? Is my machine ahead of them, or are they so flexible that they can copy anything? The computers work essentially in according with the diagram......

<sup>5</sup> 1948-1949
(299/360) (page 2992)     
or not information was being generated inside a box; for the axiom that information cannot be created is obviously related to my idea of an absolute system. I asked him, suppose someone produced a black box + claimed that information, contrary to Shannon's dictum that it cannot be created, was being created: what fundamental test would he apply to the box to settle the question? He replied that the basal concepts demanded an input into some physical transducer [his word] and an output. He would examine the transducer state by state to see if any state was ever followed by more than one subsequent state!    
